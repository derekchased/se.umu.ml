{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Welcome to Assingment  3 of Machine Learning Course 5DV194! \n",
    "The task is to practice the unsupervised algorithm Principal Component Analysis (PCA) \n",
    "#### Deadline of Assignment 3:\n",
    "23 March, 2021 (23:59:59 Stockholm time)\n",
    "\n",
    "\n",
    "#### Goal \n",
    "To practice PCA for different applications (i.e., dimensionality reduction, data visualization, image compression). Additionally, to get familiar with the libraries (e.g., sklearn PCA as well as other common libraries such as pandas, matplotlib).\n",
    "#### Dataset\n",
    "MNIST dataset\n",
    "https://www.kaggle.com/c/digit-recognizer/data\n",
    " \n",
    "\n",
    "#### Grading (100 points)\n",
    "Follow this jupyter notebook file below, you will find some tasks to complete by coding or answer questions.  \n",
    "NO report is required for this assignment.\n",
    "\n",
    "Upload this jupyter notebook file \"Assignment3_PCA.ipynb\" to Cambro/Drop Box."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About PCA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal component analysis (PCA) is fundamentally a dimensionality reduction algorithm, which could be also useful for visualization, noise filtering and image compression etc.   In principal component analysis, we find a list of the principal axes (components) in the data which will be used to describe the dataset. In this assignment, we use Scikit-Learn's PCA estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import some libraries\n",
    "###### Same as in the  previous two assignmetns, you need to install these libraries at your computer if you do not have them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST data\n",
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task-1: print the list of attribute names for variable 'mnist' by using the powerful inbuilt python function 'dir()'. (5 points)\n",
    "#### If you never heard about this function, refer to https://www.w3schools.com/python/ref_func_dir.asp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[CODES HERE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task-2: based on the printed attributes in task-1, assign the value for attribute 'data' in mnist to variable X, and the value of attribute 'target' to variable y.  (10 points) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that value here does not always mean a real number, but could be an object (e.g.,array)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separate the data labels from the data set.\n",
    "#We are doing this since MNIST is a dataset with labels, while PCA is unsupervised learning method without using labels for algorithm.  The labels would be still useful for demonstrating how PCA works.\n",
    "X = [CODES HERE]\n",
    "y = [CODES HERE] # labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardizing the data. Refer to: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "standardized_data = StandardScaler().fit_transform(X)\n",
    "print(standardized_data.shape)\n",
    "data = standardized_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Basic about PCA\n",
    "##### using Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task-3: complete the codes to output co-variance matrix (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hint: co-variance matrix is : A^T * A\n",
    "# matrix multiplication using numpy\n",
    "covar_matrix = np.matmul([CODES HERE],[CODES HERE])\n",
    "print(\"The shape of variance matrix = \", covar_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of eigen vectors =  (784, 5)\n",
      "Updated shape of eigen vectors =  (5, 784)\n"
     ]
    }
   ],
   "source": [
    "from scipy.linalg import eigh\n",
    "# A parameter 'eigvals' in function 'eigh' represents the index of eignvalues in asending order\n",
    "# for example, we can generates the top 5 (779 -783)(index) eigenvalues.\n",
    "values, vectors = eigh(covar_matrix, eigvals=(779,783))  # Note that since spicy v1.5.0, use 'subset_by_index' instead of 'eigvals'\n",
    "print(\"Shape of eigen vectors = \",vectors.shape)\n",
    "\n",
    "# converting the eigen vectors into (2,d) shape for easyness of further computations\n",
    "vectors = vectors.T\n",
    "print(\"Updated shape of eigen vectors = \",vectors.shape)\n",
    "# here the vectors[1] represent the eigen vector corresponding 1st principal eigen vector\n",
    "# here the vectors[0] represent the eigen vector corresponding 2nd principal eigen vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#projecting the original data sample on the plane \n",
    "#formed by two principal eigen vectors by vector-vector multiplication.\n",
    "\n",
    "new_coordinates = np.matmul(vectors, data.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# appending label to the 2d projected data(vertical stack)\n",
    "new_coordinates = np.vstack((new_coordinates, y)).T\n",
    "# creating a new data frame for ploting the labeled points.\n",
    "dataframe = pd.DataFrame(data=new_coordinates, columns=(\"1st_principal\", \"2nd_principal\", \"3rd_principal\", \"4th_principal\",\"5th_principal\",\"label\"))\n",
    "print(dataframe.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA for dimensionality reduction and data visualization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the data in 784 dimensions as prepared above, we can use PCA to project them to a more manageable number of dimensions (say two). Refer to https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task-4: initialize a PCA, and meanwhile specify number of components as two. Afterwards fit data into pca (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the pca \n",
    "pca = [CODES HERE] # project from 784 to 2 dimensions\n",
    "pca_data = [CODES HERE]  # fit the data into the initialized pca\n",
    "# pca_reduced will contain the 2-d projection of the given data\n",
    "print(\"shape of original shape = \", X.shape)\n",
    "print(\"shape of pca_reduced.shape = \", pca_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the string list of lables to integer list, since this is the required type for parameter c in the function plt.scatter \n",
    "label_list = [None]*len(y)\n",
    "for i in range(0, len(y)): \n",
    "    label_list[i] = int(y[i])\n",
    "\n",
    "# Plotting \n",
    "plt.scatter(pca_data[:, 0], pca_data[:, 1],\n",
    "            c=label_list, edgecolor='none', alpha=0.5,\n",
    "            cmap = plt.cm.get_cmap(\"Spectral\"))\n",
    " \n",
    "plt.xlabel('1st_principal_component')\n",
    "plt.ylabel('2nd_principal_component')\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now think about how to choose the number of components? It is a vital part of using PCA in practice. The number of components can be determined by looking at the cumulative explained variance ratio as a function of the number of components. Below let's calcuate the cumulative explained variance ratio and visualize it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Task-5:  Initialize pca, and configure the number of components (Let's see we want to keep 784 components). Note that, in task-4, you used one way to initialize the number of components you want to keep. Here provide a different way to configure the number of components.  (15 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = [CODES HERE]  # initializing the pca\n",
    "[CODES HERE]    # configure the number of components\n",
    "[CODES HERE]    # fit data into PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The amount of variance explained by each of the selected components (i.e., variance/eignvalue for each component).\n",
    "percentage_var_explained = pca.explained_variance_ / np.sum(pca.explained_variance_);\n",
    "cum_var_explained = np.cumsum(percentage_var_explained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the PCA spectrum (n_component v.s. variance)\n",
    "plt.figure(1, figsize=(6, 4))\n",
    "plt.clf()\n",
    "plt.plot(cum_var_explained, linewidth=2)\n",
    "plt.axis('tight')\n",
    "plt.grid()\n",
    "plt.xlabel('n_components')\n",
    "plt.ylabel('Cumulative_explained_variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above the curve quantifies how much of the total, 784-dimensional variance is contained within the first N components.  Obviously here our two-dimensional projection loses a lot of information (as measured by the explained variance) and that we definitely need higher number of components to retain higher vairance (for example, if we keep the top 300 components or even 200 components, the variance may be already high enough to meet your application need.  Anyway, looking at this plot for a high-dimensional dataset can help you understand the level of redundancy present in multiple observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  PCA for image compression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Task-6:  Repeat what you already did above, initialize a PCA, assign the whole size of feature space to pca.n_components, fit data to pca.  (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = [CODES HERE]  # initializing the pca\n",
    "[CODES HERE]  # fit data into pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Task-7:  initialize another PCA with  0 < n_components < 1 (say 0.95) and fit data into pca in order to apply pca on the data, assign the output to variable X_reduced (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = [CODES HERE]  \n",
    "X_reduced = [CODES HERE]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task-8 - question: what is the difference when the parameter n_components >1  and when  0 < n_components < 1  (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task-9 - question: what is the difference between the function pca.fit( ) and pca.fit_transform( ) (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task-10: print out the number of components retained in above pca model.  (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[CODES HERE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task-11: again initialize a PCA with  n_components = the output number from task-10, and fit data into this newly initialized pca (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = [CODES HERE]\n",
    "X_reduced = [CODES HERE]\n",
    "X_recovered = pca.inverse_transform(X_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we are gonna visualize the original data and compressed data by PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_digits(instances, images_per_row=5, **options):\n",
    "    size = 28\n",
    "    images_per_row = min(len(instances), images_per_row)\n",
    "    images = [instance.reshape(size,size) for instance in instances]\n",
    "    n_rows = (len(instances) - 1) // images_per_row + 1\n",
    "    row_images = []\n",
    "    n_empty = n_rows * images_per_row - len(instances)\n",
    "    images.append(np.zeros((size, size * n_empty)))\n",
    "    for row in range(n_rows):\n",
    "        rimages = images[row * images_per_row : (row + 1) * images_per_row]\n",
    "        row_images.append(np.concatenate(rimages, axis=1))\n",
    "    image = np.concatenate(row_images, axis=0)\n",
    "    plt.imshow(image, cmap = matplotlib.cm.binary, **options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 4))\n",
    "plt.subplot(121)\n",
    "plot_digits(X[::2100])\n",
    "plt.title(\"Original\", fontsize=16)\n",
    "plt.subplot(122)\n",
    "plot_digits(X_recovered[::2100])\n",
    "plt.title(\"Compressed\", fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task-12:  tell me the good points in this assignment and limited points which can be further improved.  (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Congratulations if you managed to run through above! \n",
    "#### Hope you find something interesting from this assignment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
